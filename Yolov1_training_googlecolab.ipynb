{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "dPc1NqYusnLM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from torch.utils.data import DataLoader\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import pandas as pd\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "QZpdZuYZAXgy"
      },
      "outputs": [],
      "source": [
        "# cr√©er deux dossier images et labels dans home, uploader les images et labels dans ces dossiers \n",
        "# et uploader le csv 100examples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "5QP0mQ3ZyOBU"
      },
      "outputs": [],
      "source": [
        "os.chdir('/home/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "fuQjGvLqstMj",
        "outputId": "1b38393f-fe21-44da-9a7e-9cd8f5c37423"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'   \\ndef test(S=7, B=2, C=20):\\n    model = Yolov1(split_size = S, num_boxes = B, num_classes = C)\\n    x = torch.randn([2, 3, 448, 448])\\n    print(model(x).shape)\\n\\ntest()\\n'"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#model.py\n",
        "\"\"\" \n",
        "YOLOv1 architecture config: conv layers\n",
        "Tuple structure: (kernel_size, filters, stride, padding). The padding is calculated by hand\n",
        "\"M\" for maxpooling: stride 2x2, kernel 2x2\n",
        "List structure: tuples and int, the number of repeats\n",
        "\"\"\"\n",
        "\n",
        "architecture_config = [\n",
        "    (7, 64, 2, 3),\n",
        "    \"M\",\n",
        "    (3, 192, 1, 1),\n",
        "    \"M\",\n",
        "    (1, 128, 1, 0),\n",
        "    (3, 256, 1, 1),\n",
        "    (1, 256, 1, 0),\n",
        "    (3, 512, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 256, 1, 0), (3, 512, 1, 1), 4],\n",
        "    (1, 512, 1, 0),\n",
        "    (3, 1024, 1, 1),\n",
        "    \"M\",\n",
        "    [(1, 512, 1, 0), (3, 1024, 1, 1), 2],\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 2, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "    (3, 1024, 1, 1),\n",
        "]\n",
        "\n",
        "class CNNBlock(nn.Module): # a general CNN block class that we'll often use\n",
        "    def __init__(self, in_channels, out_channels, **kwargs):\n",
        "        super(CNNBlock, self).__init__()\n",
        "        self.conv = nn.Conv2d(in_channels, out_channels, bias= False, **kwargs)\n",
        "        self.batchnorm = nn.BatchNorm2d(out_channels) # Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift\n",
        "        self.leakyrelu = nn.LeakyReLU(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.leakyrelu(self.batchnorm(self.conv(x)))\n",
        "    \n",
        "\n",
        "class Yolov1(nn.Module):\n",
        "    def __init__(self, in_channels=3, **kwargs): # in_channels : 3 for RGB\n",
        "        super(Yolov1, self).__init__()\n",
        "        self.architecture = architecture_config\n",
        "        self.in_channels = in_channels\n",
        "        self.darknet = self._create_conv_layers(self.architecture) # build from architecture\n",
        "        self.fcs = self._create_fcs(**kwargs) # fcs for fully connected layers\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.darknet(x)\n",
        "        return self.fcs(torch.flatten(x, start_dim=1))\n",
        "    \n",
        "    def _create_conv_layers(self, architecture): # create the darknet architecture\n",
        "        layers= []\n",
        "        in_channels = self.in_channels\n",
        "\n",
        "        for layer in architecture:\n",
        "            if type(layer) == tuple:\n",
        "                layers+= [CNNBlock(in_channels=in_channels, out_channels=layer[1], kernel_size= layer[0],\n",
        "                                  stride = layer[2], padding = layer[3]) ]\n",
        "                in_channels = layer[1]\n",
        "            \n",
        "            elif type(layer) == str:\n",
        "                layers+= [nn.MaxPool2d(kernel_size=2, stride = 2)]\n",
        "            \n",
        "            elif type(layer) == list:\n",
        "                conv1 = layer[0] # tuple\n",
        "                conv2 = layer[1] # tuple\n",
        "                num_repeats = layer[2]\n",
        "\n",
        "                for _ in range(num_repeats):\n",
        "                    layers+= [CNNBlock(in_channels=in_channels, out_channels=conv1[1], kernel_size= conv1[0],\n",
        "                                  stride = conv1[2], padding = conv1[3]) ]\n",
        "                    layers+= [CNNBlock(in_channels=conv1[1], out_channels=conv2[1], kernel_size= conv2[0],\n",
        "                                  stride = conv2[2], padding = conv2[3]) ]\n",
        "                    \n",
        "                in_channels = conv2[1]\n",
        "\n",
        "        return nn.Sequential(*layers) # unpack the list layers and convert it to nn.Sequential\n",
        "    \n",
        "    def _create_fcs(self, split_size, num_boxes, num_classes): # create the fully connected layers architecture\n",
        "        S, B, C= split_size, num_boxes, num_classes\n",
        "\n",
        "        return nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(1024*S*S, 496), # 4096 in the original paper, but need to decrease that on a small computer beacause of small VRAM\n",
        "            nn.Dropout(0.0),\n",
        "            nn.LeakyReLU(0.1),\n",
        "            nn.Linear(496, S*S*(C+B*5)) # reshape by (S, S, C+B*5)\n",
        "        )\n",
        "\n",
        "# quickly test of the Yolov1 model\n",
        "\"\"\"   \n",
        "def test(S=7, B=2, C=20):\n",
        "    model = Yolov1(split_size = S, num_boxes = B, num_classes = C)\n",
        "    x = torch.randn([2, 3, 448, 448])\n",
        "    print(model(x).shape)\n",
        "\n",
        "test()\n",
        "\"\"\"\n",
        "\n",
        "             \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "WwHob7CXs9lV",
        "outputId": "d8e69f50-bdda-4e60-8ea9-5df538d0b67f"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef test():\\n    predictions = torch.randn([10, 7, 7, 30])\\n    target = torch.randn([10, 7, 7, 30])\\n    Loss = YoloLoss()\\n    print(Loss(predictions, target))\\n\\ntest()\\n'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#loss.py\n",
        "class YoloLoss(nn.Module):\n",
        "    def __init__(self, S=7, B=2, C=20) :\n",
        "        super(YoloLoss, self).__init__()\n",
        "        self.mse = nn.MSELoss(reduction=\"sum\")\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "        self.lambda_noobj = 0.5\n",
        "        self.lambda_coord = 5\n",
        "\n",
        "    def forward(self, predictions, target):\n",
        "        predictions = predictions.reshape(-1, self.S, self.S, self.C + self.B*5) # we predict two box per cell, dim = (N, S, S, 30) N batch_size\n",
        "\n",
        "        iou_b1 = intersection_over_union(predictions[..., 21:25], target[..., 21:25]) \n",
        "        iou_b2 = intersection_over_union(predictions[..., 26:30], target[..., 21:25])\n",
        "        ious = torch.cat([iou_b1.unsqueeze(0), iou_b2.unsqueeze(0)], dim=0) # seen like a tab with two columns\n",
        "        iou_maxes, bestbox = torch.max(ious, dim=0) # bestbox is the argmax, 0 (if the first box, corresponds to the first tab's col is the best) or 1 (else)\n",
        "        exists_box = target[...,20].unsqueeze(3) # identity of obj i (1 if there is obj and 0 if not)\n",
        "\n",
        "        \n",
        "        #---------------------for box loss-------------------------\n",
        "        box_predictions = exists_box* (bestbox*predictions[..., 26:30] + (1- bestbox)* predictions[..., 21:25])\n",
        "        box_targets = exists_box * target[..., 21:25]\n",
        "\n",
        "        box_predictions[...,2:4] = torch.sign(box_predictions[...,2:4]) * torch.sqrt(torch.abs(box_predictions[...,2:4] + 1e-6)) # take the sqrt of the predicted weight and high cause we predict the square (see the paper)\n",
        "        box_targets[..., 2:4] = torch.sqrt(box_targets[..., 2:4])\n",
        "\n",
        "        # box predictions and targets dim : (N, S, S, 4) ->(flatten, end_dim = -2)-> (N*S*S, 4)\n",
        "        box_loss = self.mse(torch.flatten(box_predictions, end_dim = -2), torch.flatten(box_targets, end_dim = -2))\n",
        "\n",
        "        \n",
        "        #--------------------- for obj loss-------------------------------\n",
        "        pred_box = (bestbox*predictions[..., 25:26] + (1- bestbox)*predictions[..., 20:21])\n",
        "\n",
        "        # pred_box flattened dim : (N*S*S, 1)\n",
        "        object_loss = self.mse(torch.flatten(exists_box*pred_box), torch.flatten(exists_box*target[..., 20:21]))\n",
        "\n",
        "        \n",
        "        #------------------------- for no obj loss---------------------------------------\n",
        "        # predictions flattened dim : (N, S, S, 1) -> (N, S*S*1)\n",
        "        no_object_loss = self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 20:21], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1),\n",
        "        )\n",
        "\n",
        "        no_object_loss += self.mse(\n",
        "            torch.flatten((1 - exists_box) * predictions[..., 25:26], start_dim=1),\n",
        "            torch.flatten((1 - exists_box) * target[..., 20:21], start_dim=1)\n",
        "        )\n",
        "\n",
        "        \n",
        "        #-------------------------for class loss----------------------------------------------\n",
        "        # (N,S,S,20) -> (N*S*S, 20)\n",
        "        class_loss = self.mse(\n",
        "            torch.flatten(exists_box * predictions[..., :20], end_dim=-2,),\n",
        "            torch.flatten(exists_box * target[..., :20], end_dim=-2,),\n",
        "        )\n",
        "\n",
        "        loss = (\n",
        "            self.lambda_coord * box_loss  # first two rows in paper\n",
        "            + object_loss  # third row in paper\n",
        "            + self.lambda_noobj * no_object_loss  # forth row\n",
        "            + class_loss  # fifth row\n",
        "        )\n",
        "\n",
        "        return loss\n",
        "    \n",
        "# quick test to make sure that everything work\n",
        "\"\"\"\n",
        "def test():\n",
        "    predictions = torch.randn([10, 7, 7, 30])\n",
        "    target = torch.randn([10, 7, 7, 30])\n",
        "    Loss = YoloLoss()\n",
        "    print(Loss(predictions, target))\n",
        "\n",
        "test()\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        },
        "id": "-3b8kv5ttNJX",
        "outputId": "fbb784a0-092c-4421-f008-77feac341e37"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\"\\ncsv_file= 'data/8examples.csv'\\nimage_dir= 'data/images'\\nlabel_dir = 'data/labels'\\nindex= 2\\n\\ndef test():\\n    dataloader = VOCDataset(csv_file, image_dir, label_dir)\\n    l = dataloader.__len__()\\n    image, label_matrix = dataloader.__getitem__(index)\\n\\n    print('len csv_file :', l, 'label_matrix dim :', label_matrix.shape)\\n    print('sum label_matrix :', sum(label_matrix.flatten()))\\n    print(np.array(image).shape)\\n\\ntest()\\n\""
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# dataset.py\n",
        "class VOCDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, csv_file, img_dir, label_dir, S=7, B=2, C=20, transform= None): # transform for data augmentation\n",
        "        self.annotations = pd.read_csv(csv_file)\n",
        "        self.img_dir = img_dir\n",
        "        self.label_dir = label_dir\n",
        "        self.transform = transform\n",
        "        self.S = S\n",
        "        self.B = B\n",
        "        self.C = C\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.annotations)\n",
        "    \n",
        "    def __getitem__(self, index):\n",
        "        # ------------ processing labels ----------------\n",
        "        label_path = os.path.join(self.label_dir, self.annotations.iloc[index, 1]) # the label filename is in the second column of the csv_file\n",
        "        boxes = []\n",
        "        with open(label_path) as f:\n",
        "            for label in f.readlines():\n",
        "                class_label, x, y, width, height = [float(x) if float(x)!=int(float(x)) else int(x)\n",
        "                                                    for x in label.replace(\"\\n\", \"\").split()]\n",
        "                \n",
        "                boxes.append([class_label, x, y, width, height])\n",
        "        \n",
        "        # ------------- processing images ----------------\n",
        "        img_path = os.path.join(self.img_dir, self.annotations.iloc[index, 0])\n",
        "        image = Image.open(img_path)\n",
        "        boxes = torch.tensor(boxes)\n",
        "\n",
        "        if self.transform:\n",
        "            image, boxes = self.transform(image, boxes)\n",
        "\n",
        "        label_matrix = torch.zeros((self.S, self.S, self.C + 5*self.B))\n",
        "        for box in boxes:\n",
        "            class_label, x, y, width, height = box.tolist()\n",
        "            class_label = int(class_label)\n",
        "            i, j = int(self.S*y), int(self.S*x) # to know the cell of the box in images, x, y are normalized to be in [0,1] so multiply by the number of cells to know the corresponding cell of the box\n",
        "            x_cell, y_cell = self.S*x - j, self.S*y - i \n",
        "            width_cell, height_cell = (width* self.S, height* self.S)\n",
        "            \n",
        "            \n",
        "            if label_matrix[i, j, 20] == 0: # if there is no object in the calculated cell i, j\n",
        "                label_matrix[i, j, 20] = 1\n",
        "                box_coordinates = torch.tensor([x_cell, y_cell, width_cell, height_cell])\n",
        "                #box_coordinates = torch.tensor([x_cell, y_cell, width, height])\n",
        "                label_matrix[i, j, 21:25] = box_coordinates\n",
        "                label_matrix[i, j, class_label] = 1 # probability of the presence of object in that cell is equal to 1\n",
        "\n",
        "        return image, label_matrix\n",
        "\n",
        "\"\"\"\n",
        "csv_file= 'data/8examples.csv'\n",
        "image_dir= 'data/images'\n",
        "label_dir = 'data/labels'\n",
        "index= 2\n",
        "\n",
        "def test():\n",
        "    dataloader = VOCDataset(csv_file, image_dir, label_dir)\n",
        "    l = dataloader.__len__()\n",
        "    image, label_matrix = dataloader.__getitem__(index)\n",
        "\n",
        "    print('len csv_file :', l, 'label_matrix dim :', label_matrix.shape)\n",
        "    print('sum label_matrix :', sum(label_matrix.flatten()))\n",
        "    print(np.array(image).shape)\n",
        "\n",
        "test()\n",
        "\"\"\"\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "Q33VinPitYum"
      },
      "outputs": [],
      "source": [
        "#utils.py\n",
        "def intersection_over_union(boxes_preds, boxes_labels, box_format=\"midpoint\"):\n",
        "    \"\"\"\n",
        "    Calculates intersection over union\n",
        "    Parameters:\n",
        "        boxes_preds (tensor): Predictions of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        boxes_labels (tensor): Correct labels of Bounding Boxes (BATCH_SIZE, 4)\n",
        "        box_format (str): midpoint/corners, if boxes (x,y,w,h) or (x1,y1,x2,y2)\n",
        "    Returns:\n",
        "        tensor: Intersection over union for all examples\n",
        "    \"\"\"\n",
        "\n",
        "    if box_format == \"midpoint\":\n",
        "        box1_x1 = boxes_preds[..., 0:1] - boxes_preds[..., 2:3] / 2\n",
        "        box1_y1 = boxes_preds[..., 1:2] - boxes_preds[..., 3:4] / 2\n",
        "        box1_x2 = boxes_preds[..., 0:1] + boxes_preds[..., 2:3] / 2\n",
        "        box1_y2 = boxes_preds[..., 1:2] + boxes_preds[..., 3:4] / 2\n",
        "        box2_x1 = boxes_labels[..., 0:1] - boxes_labels[..., 2:3] / 2\n",
        "        box2_y1 = boxes_labels[..., 1:2] - boxes_labels[..., 3:4] / 2\n",
        "        box2_x2 = boxes_labels[..., 0:1] + boxes_labels[..., 2:3] / 2\n",
        "        box2_y2 = boxes_labels[..., 1:2] + boxes_labels[..., 3:4] / 2\n",
        "\n",
        "    if box_format == \"corners\":\n",
        "        box1_x1 = boxes_preds[..., 0:1]\n",
        "        box1_y1 = boxes_preds[..., 1:2]\n",
        "        box1_x2 = boxes_preds[..., 2:3]\n",
        "        box1_y2 = boxes_preds[..., 3:4]  # (N, 1)\n",
        "        box2_x1 = boxes_labels[..., 0:1]\n",
        "        box2_y1 = boxes_labels[..., 1:2]\n",
        "        box2_x2 = boxes_labels[..., 2:3]\n",
        "        box2_y2 = boxes_labels[..., 3:4]\n",
        "\n",
        "    x1 = torch.max(box1_x1, box2_x1)\n",
        "    y1 = torch.max(box1_y1, box2_y1)\n",
        "    x2 = torch.min(box1_x2, box2_x2)\n",
        "    y2 = torch.min(box1_y2, box2_y2)\n",
        "\n",
        "    # .clamp(0) is for the case when they do not intersect\n",
        "    intersection = (x2 - x1).clamp(0) * (y2 - y1).clamp(0)\n",
        "\n",
        "    box1_area = abs((box1_x2 - box1_x1) * (box1_y2 - box1_y1))\n",
        "    box2_area = abs((box2_x2 - box2_x1) * (box2_y2 - box2_y1))\n",
        "\n",
        "    return intersection / (box1_area + box2_area - intersection + 1e-6)\n",
        "\n",
        "def non_max_suppression(bboxes, iou_threshold, threshold, box_format=\"corners\"):\n",
        "    \"\"\"\n",
        "    Does Non Max Suppression given bboxes\n",
        "    Parameters:\n",
        "        bboxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [class_pred, prob_score, x1, y1, x2, y2]\n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        threshold (float): threshold to remove predicted bboxes (independent of IoU) \n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "    Returns:\n",
        "        list: bboxes after performing NMS given a specific IoU threshold\n",
        "    \"\"\"\n",
        "\n",
        "    assert type(bboxes) == list\n",
        "\n",
        "    bboxes = [box for box in bboxes if box[1] > threshold] # select only boxes where an object is detected\n",
        "    bboxes = sorted(bboxes, key=lambda x: x[1], reverse=True) \n",
        "    bboxes_after_nms = []\n",
        "\n",
        "    while bboxes:\n",
        "        chosen_box = bboxes.pop(0)\n",
        "\n",
        "        bboxes = [\n",
        "            box\n",
        "            for box in bboxes\n",
        "            if box[0] != chosen_box[0]\n",
        "            or intersection_over_union(\n",
        "                torch.tensor(chosen_box[2:]),\n",
        "                torch.tensor(box[2:]),\n",
        "                box_format=box_format,\n",
        "            )\n",
        "            < iou_threshold\n",
        "        ]\n",
        "\n",
        "        bboxes_after_nms.append(chosen_box)\n",
        "\n",
        "    return bboxes_after_nms\n",
        "\n",
        "\n",
        "def mean_average_precision(\n",
        "    pred_boxes, true_boxes, iou_threshold=0.5, box_format=\"midpoint\", num_classes=20\n",
        "):\n",
        "    \"\"\"\n",
        "    Calculates mean average precision \n",
        "    Parameters:\n",
        "        pred_boxes (list): list of lists containing all bboxes with each bboxes\n",
        "        specified as [train_idx, class_prediction, prob_score, x1, y1, x2, y2]\n",
        "        true_boxes (list): Similar as pred_boxes except all the correct ones \n",
        "        iou_threshold (float): threshold where predicted bboxes is correct\n",
        "        box_format (str): \"midpoint\" or \"corners\" used to specify bboxes\n",
        "        num_classes (int): number of classes\n",
        "    Returns:\n",
        "        float: mAP value across all classes given a specific IoU threshold \n",
        "    \"\"\"\n",
        "\n",
        "    # list storing all AP for respective classes\n",
        "    average_precisions = []\n",
        "\n",
        "    # used for numerical stability later on\n",
        "    epsilon = 1e-6\n",
        "\n",
        "    for c in range(num_classes):\n",
        "        detections = []\n",
        "        ground_truths = []\n",
        "\n",
        "        # Go through all predictions and targets,\n",
        "        # and only add the ones that belong to the\n",
        "        # current class c\n",
        "        for detection in pred_boxes:\n",
        "            if detection[1] == c:\n",
        "                detections.append(detection)\n",
        "\n",
        "        for true_box in true_boxes:\n",
        "            if true_box[1] == c:\n",
        "                ground_truths.append(true_box)\n",
        "\n",
        "        # find the amount of bboxes for each training example\n",
        "        # Counter here finds how many ground truth bboxes we get\n",
        "        # for each training example, so let's say img 0 has 3,\n",
        "        # img 1 has 5 then we will obtain a dictionary with:\n",
        "        # amount_bboxes = {0:3, 1:5}\n",
        "        amount_bboxes = Counter([gt[0] for gt in ground_truths])\n",
        "\n",
        "        # We then go through each key, val in this dictionary\n",
        "        # and convert to the following (w.r.t same example):\n",
        "        # ammount_bboxes = {0:torch.tensor[0,0,0], 1:torch.tensor[0,0,0,0,0]}\n",
        "        for key, val in amount_bboxes.items():\n",
        "            amount_bboxes[key] = torch.zeros(val)\n",
        "\n",
        "        # sort by box probabilities which is index 2\n",
        "        detections.sort(key=lambda x: x[2], reverse=True)\n",
        "        TP = torch.zeros((len(detections)))\n",
        "        FP = torch.zeros((len(detections)))\n",
        "        total_true_bboxes = len(ground_truths)\n",
        "        \n",
        "        # If none exists for this class then we can safely skip\n",
        "        if total_true_bboxes == 0:\n",
        "            continue\n",
        "\n",
        "        for detection_idx, detection in enumerate(detections):\n",
        "            # Only take out the ground_truths that have the same\n",
        "            # training idx as detection\n",
        "            ground_truth_img = [\n",
        "                bbox for bbox in ground_truths if bbox[0] == detection[0]\n",
        "            ]\n",
        "\n",
        "            num_gts = len(ground_truth_img)\n",
        "            best_iou = 0\n",
        "\n",
        "            for idx, gt in enumerate(ground_truth_img):\n",
        "                iou = intersection_over_union(\n",
        "                    torch.tensor(detection[3:]),\n",
        "                    torch.tensor(gt[3:]),\n",
        "                    box_format=box_format,\n",
        "                )\n",
        "\n",
        "                if iou > best_iou:\n",
        "                    best_iou = iou\n",
        "                    best_gt_idx = idx\n",
        "\n",
        "            if best_iou > iou_threshold:\n",
        "                # only detect ground truth detection once\n",
        "                if amount_bboxes[detection[0]][best_gt_idx] == 0:\n",
        "                    # true positive and add this bounding box to seen\n",
        "                    TP[detection_idx] = 1\n",
        "                    amount_bboxes[detection[0]][best_gt_idx] = 1\n",
        "                else:\n",
        "                    FP[detection_idx] = 1\n",
        "\n",
        "            # if IOU is lower then the detection is a false positive\n",
        "            else:\n",
        "                FP[detection_idx] = 1\n",
        "\n",
        "        TP_cumsum = torch.cumsum(TP, dim=0)\n",
        "        FP_cumsum = torch.cumsum(FP, dim=0)\n",
        "        recalls = TP_cumsum / (total_true_bboxes + epsilon)\n",
        "        precisions = torch.divide(TP_cumsum, (TP_cumsum + FP_cumsum + epsilon))\n",
        "        precisions = torch.cat((torch.tensor([1]), precisions))\n",
        "        recalls = torch.cat((torch.tensor([0]), recalls))\n",
        "        # torch.trapz for numerical integration\n",
        "        average_precisions.append(torch.trapz(precisions, recalls))\n",
        "\n",
        "    return sum(average_precisions) / len(average_precisions)\n",
        "\n",
        "\n",
        "def get_bboxes(\n",
        "    loader,\n",
        "    model,\n",
        "    iou_threshold,\n",
        "    threshold,\n",
        "    pred_format=\"cells\",\n",
        "    box_format=\"midpoint\",\n",
        "    device=\"cuda\",\n",
        "):\n",
        "    all_pred_boxes = []\n",
        "    all_true_boxes = []\n",
        "\n",
        "    # make sure model is in eval before get bboxes\n",
        "    #model.eval()\n",
        "    train_idx = 0\n",
        "\n",
        "    for batch_idx, (x, labels) in enumerate(loader):\n",
        "        x = x.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            predictions = model(x)\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        true_bboxes = cellboxes_to_boxes(labels)\n",
        "        bboxes = cellboxes_to_boxes(predictions)\n",
        "\n",
        "        for idx in range(batch_size):\n",
        "            nms_boxes = non_max_suppression(\n",
        "                bboxes[idx],\n",
        "                iou_threshold=iou_threshold,\n",
        "                threshold=threshold,\n",
        "                box_format=box_format,\n",
        "            )\n",
        "\n",
        "            for nms_box in nms_boxes:\n",
        "                all_pred_boxes.append([train_idx] + nms_box)\n",
        "\n",
        "            for box in true_bboxes[idx]:\n",
        "                # many will get converted to 0 pred\n",
        "                if box[1] > threshold:\n",
        "                    all_true_boxes.append([train_idx] + box)\n",
        "\n",
        "            train_idx += 1\n",
        "\n",
        "    #model.train()\n",
        "    return all_pred_boxes, all_true_boxes\n",
        "\n",
        "\n",
        "\n",
        "def convert_cellboxes(predictions, S=7):\n",
        "    \"\"\"\n",
        "    Converts bounding boxes output from Yolo with\n",
        "    an image split size of S into entire image ratios\n",
        "    rather than relative to cell ratios. Tried to do this\n",
        "    vectorized, but this resulted in quite difficult to read\n",
        "    code... Use as a black box? Or implement a more intuitive,\n",
        "    using 2 for loops iterating range(S) and convert them one\n",
        "    by one, resulting in a slower but more readable implementation.\n",
        "    \"\"\"\n",
        "\n",
        "    predictions = predictions.to(\"cpu\")\n",
        "    batch_size = predictions.shape[0]\n",
        "    predictions = predictions.reshape(batch_size, 7, 7, 30)\n",
        "    bboxes1 = predictions[..., 21:25]\n",
        "    bboxes2 = predictions[..., 26:30]\n",
        "    scores = torch.cat(\n",
        "        (predictions[..., 20].unsqueeze(0), predictions[..., 25].unsqueeze(0)), dim=0\n",
        "    )\n",
        "    best_box = scores.argmax(0).unsqueeze(-1)\n",
        "    best_boxes = bboxes1 * (1 - best_box) + best_box * bboxes2\n",
        "    \n",
        "\n",
        "    cell_indices = torch.arange(7).repeat(batch_size, 7, 1).unsqueeze(-1)\n",
        "    x = 1 / S * (best_boxes[..., :1] + cell_indices)\n",
        "    y = 1 / S * (best_boxes[..., 1:2] + cell_indices.permute(0, 2, 1, 3))\n",
        "    w_y = 1 / S * best_boxes[..., 2:4]\n",
        "    #w_y = best_boxes[..., 2:4]\n",
        "    converted_bboxes = torch.cat((x, y, w_y), dim=-1)\n",
        "    predicted_class = predictions[..., :20].argmax(-1).unsqueeze(-1)\n",
        "    best_confidence = torch.max(predictions[..., 20], predictions[..., 25]).unsqueeze(\n",
        "        -1\n",
        "    )\n",
        "    converted_preds = torch.cat(\n",
        "        (predicted_class, best_confidence, converted_bboxes), dim=-1\n",
        "    )\n",
        "\n",
        "    return converted_preds\n",
        "\n",
        "\n",
        "def cellboxes_to_boxes(out, S=7):\n",
        "    converted_pred = convert_cellboxes(out).reshape(out.shape[0], S * S, -1)\n",
        "    converted_pred[..., 0] = converted_pred[..., 0].long()\n",
        "    all_bboxes = []\n",
        "\n",
        "    for ex_idx in range(out.shape[0]):\n",
        "        bboxes = []\n",
        "\n",
        "        for bbox_idx in range(S * S):\n",
        "            bboxes.append([x.item() for x in converted_pred[ex_idx, bbox_idx, :]])\n",
        "        all_bboxes.append(bboxes)\n",
        "\n",
        "    return all_bboxes\n",
        "\n",
        "def save_checkpoint(state, filename=\"my_checkpoint.pth.tar\"):\n",
        "    print(\"=> Saving checkpoint...\")\n",
        "    torch.save(state, filename)\n",
        "    print(\"Successfully saved !\")\n",
        "\n",
        "\n",
        "def load_checkpoint(checkpoint, model, optimizer):\n",
        "    print(\"=> Loading checkpoint...\")\n",
        "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
        "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
        "    print(\"Successfully loaded !\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "9XiiBMpStoZa"
      },
      "outputs": [],
      "source": [
        "seed = 4567\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 2e-5\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "batch_size = 16\n",
        "weight_decay = 0\n",
        "epochs = 50\n",
        "num_workers = 2\n",
        "pin_memory = True\n",
        "load_model = False\n",
        "load_model_file = \"Yolov1_100images_50epochs.path.tar\"\n",
        "img_dir = 'images'\n",
        "label_dir = 'labels'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "LHC5mxIQtic5"
      },
      "outputs": [],
      "source": [
        "class Compose(object):\n",
        "    def __init__(self, transforms):\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __call__(self, img, bboxes):\n",
        "        for t in self.transforms:\n",
        "            img, bboxes = t(img), bboxes\n",
        "\n",
        "        return img, bboxes\n",
        "\n",
        "\n",
        "transform = Compose([transforms.Resize((448, 448)), transforms.ToTensor()])\n",
        "\n",
        "def train_fn(train_loader, model, optimizer, loss_fn):\n",
        "    loop = tqdm(train_loader, leave = True)\n",
        "    mean_loss = []\n",
        "    model.train()\n",
        "\n",
        "    for batch_idx, (x,y) in enumerate(loop):\n",
        "        x, y = x.to(device), y.to(device)\n",
        "        out = model(x)\n",
        "        loss = loss_fn(out, y)\n",
        "        ll = loss.item()\n",
        "        mean_loss.append(ll)\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # update the progress bar\n",
        "        loop.set_postfix(loss = loss.item())\n",
        "\n",
        "\n",
        "def main():\n",
        "    model = Yolov1(split_size = 7, num_boxes = 2, num_classes=20).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "    loss_fn = YoloLoss()\n",
        "    \n",
        "\n",
        "    if load_model:\n",
        "        load_checkpoint(torch.load(load_model_file), model, optimizer)\n",
        "\n",
        "    train_dataset = VOCDataset('/home/100examples.csv', transform=transform, img_dir=img_dir, label_dir=label_dir)\n",
        "    \n",
        "    #test_dataset = VOCDataset('data/test.csv', transform=transform, img_dir=img_dir, label_dir=label_dir)\n",
        "    \n",
        "    train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory,\n",
        "                              shuffle=False, drop_last=True) # drop_last= True: if the last batch'size < batch_size, we'll ignore it\n",
        "    \n",
        "    \"\"\"test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, num_workers=num_workers, pin_memory=pin_memory,\n",
        "                              shuffle=False, drop_last=False) # drop_last : if the last batch'size < batch_size, we'll ignore it\"\"\"\n",
        "    \n",
        "    for epoch in range(epochs):    \n",
        "        print('epoch n¬∞', epoch+1)\n",
        "        pred_boxes, target_boxes = get_bboxes(train_loader, model, iou_threshold=0.5, threshold=0.4, device=device)\n",
        "        mean_avg_prec = mean_average_precision(pred_boxes, target_boxes, iou_threshold=0.5, box_format='midpoint')\n",
        "        print(f'Train mAP: {mean_avg_prec}')\n",
        "        \n",
        "        train_fn(train_loader, model, optimizer, loss_fn)\n",
        "\n",
        "        # to save the model\n",
        "        if epoch==epochs-1 or mean_avg_prec>0.9: \n",
        "            checkpoint = {\n",
        "                \"state_dict\": model.state_dict(),\n",
        "                \"optimizer\": optimizer.state_dict(),\n",
        "            }\n",
        "            save_checkpoint(checkpoint, filename=\"Yolov1_100images_optim_mAP_0_90.path.tar\")\n",
        "            time.sleep(10)\n",
        "            break\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gjySYYRnuGRL",
        "outputId": "56ab751a-f64f-49cc-ff98-6ee3391d9bb3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 1\n",
            "Train mAP: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:18<00:00, 43.12s/it, loss=786]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 2\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:12<00:00, 42.00s/it, loss=451]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.0\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:12<00:00, 42.05s/it, loss=322]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 4\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.011484049260616302\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:14<00:00, 42.34s/it, loss=259]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 5\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.006903525907546282\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:13<00:00, 42.31s/it, loss=203]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 6\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.05285713076591492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:14<00:00, 42.41s/it, loss=161]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 7\n",
            "Train mAP: 0.13054294884204865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:12<00:00, 42.16s/it, loss=129]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.24769273400306702\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:16<00:00, 42.73s/it, loss=105]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 9\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.4340856075286865\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:13<00:00, 42.30s/it, loss=87.2]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 10\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.5911478400230408\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:10<00:00, 41.80s/it, loss=73.6]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 11\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.7088414430618286\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:13<00:00, 42.23s/it, loss=62.8]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 12\n",
            "Train mAP: 0.7558184862136841\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:15<00:00, 42.51s/it, loss=54.1]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 13\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.8355050683021545\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:15<00:00, 42.58s/it, loss=46.6]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch n¬∞ 14\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train mAP: 0.9047496914863586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 6/6 [04:12<00:00, 42.10s/it, loss=40.5]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=> Saving checkpoint...\n",
            "Successfully saved !\n"
          ]
        }
      ],
      "source": [
        "main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "hh5ZBjWYygbY"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
